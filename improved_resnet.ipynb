{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7UlbYJaSIeO",
        "outputId": "6d5b235d-e161-454a-9efb-4a33dde0a738"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "#import torch libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NhhI4TLSoxh",
        "outputId": "2ceaf2cd-50cf-44f7-ec85-aa310d9865f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Qualcomm-DL-Hackathon'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 30.68 MiB | 14.94 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/Prashant-AV/Qualcomm-DL-Hackathon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RJ9QYzaSSsV9"
      },
      "outputs": [],
      "source": [
        "# extract the content\n",
        "extract_dir = \"/content/Train\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "#open and extract the zip file\n",
        "for i in range(1,3):\n",
        "  with zipfile.ZipFile(f\"/content/Qualcomm-DL-Hackathon/train/images part-{i}.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KjanpXuTTON-"
      },
      "outputs": [],
      "source": [
        "train_data_csv = pd.read_csv(\"/content/Qualcomm-DL-Hackathon/train/train.csv\")\n",
        "test = pd.read_csv(\"/content/Qualcomm-DL-Hackathon/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JZrjbCXGS24H"
      },
      "outputs": [],
      "source": [
        "# extract images and split them into train and test\n",
        "\n",
        "destination_folder = \"/content/Train/train_images\"\n",
        "\n",
        "if not os.path.exists(destination_folder):\n",
        "  os.makedirs(destination_folder)\n",
        "\n",
        "for i in range(1,3):\n",
        "  source_folder = f\"/content/Train/images part-{i}\"\n",
        "  for root, dirs, files in os.walk(source_folder):\n",
        "    for file in files:\n",
        "      src_file = os.path.join(root, file)\n",
        "      shutil.copy2(src_file, destination_folder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dbKZpbUGTI_G"
      },
      "outputs": [],
      "source": [
        "#splitting datasets into train and test\n",
        "test_images = list(test[\"image_names\"])\n",
        "source_folder = f\"/content/Train/train_images\"\n",
        "destination_folder = \"/content/Train/final_test_images\"\n",
        "\n",
        "if not os.path.exists(destination_folder):\n",
        "  os.makedirs(destination_folder)\n",
        "count = 0\n",
        "for root, dirs, files in os.walk(source_folder):\n",
        "  for file in files:\n",
        "    count += 1\n",
        "    if file in test_images:\n",
        "      src_file = os.path.join(root, file)\n",
        "      shutil.move(src_file, destination_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L26HTJhdTdn1"
      },
      "outputs": [],
      "source": [
        "# initialize the res dict to construct final csv at the end\n",
        "count = 0\n",
        "res = {\"image_names\":[], \"emergency_or_not\":[]}\n",
        "for root, dirs, files in os.walk(destination_folder):\n",
        "  for file in files:\n",
        "    # if count < 1:\n",
        "    #   print(file, type(file), test_images[0],type(test_images[0]))\n",
        "    count += 1\n",
        "    res[\"image_names\"].append(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c_DlI-7ATtQd"
      },
      "outputs": [],
      "source": [
        "# Creating classes to handle image to label mapping with(for train) and without labels(for test)\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class CustomImageDatasetWithLabels(Dataset):\n",
        "    def __init__(self, csv_file, image_folder, transform=None):\n",
        "        self.labels_df = pd.read_csv(csv_file)\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_folder, self.labels_df.iloc[idx, 0])\n",
        "        image = Image.open(img_name)\n",
        "        label = int(self.labels_df.iloc[idx, 1])  # Convert label to int\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "class CustomImageDatasetWithoutLabels(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.image_names = os.listdir(image_folder)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_folder, self.image_names[idx])\n",
        "        image = Image.open(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XEiDiMhKALr_"
      },
      "outputs": [],
      "source": [
        "# Tried different combinations with color saturation, vertical flip, etc.\n",
        "# Found this combination to be better than others\n",
        "# Here, we resize the image to 256x256 along with horizontal flip and rotation of 15 degrees.\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KwhUtoO1U9H1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Paths to your CSV files and image folders\n",
        "train_csv = '/content/Qualcomm-DL-Hackathon/train/train.csv'\n",
        "train_image_folder = '/content/Train/train_images'\n",
        "\n",
        "# Initialize your datasets\n",
        "train_dataset = CustomImageDatasetWithLabels(csv_file=train_csv, image_folder=train_image_folder, transform=train_transforms)\n",
        "test_dataset = CustomImageDatasetWithoutLabels(image_folder=destination_folder, transform=test_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WLczoprf9BBS"
      },
      "outputs": [],
      "source": [
        "# Defining a function to split data into train and test sets\n",
        "def split_data(data,ratio=0.8):\n",
        "    train_size = int(ratio * len(data))\n",
        "    test_size = len(data) - train_size\n",
        "    train_data, test_data = random_split(data, [train_size, test_size])\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3Zqbqg4G9CLy"
      },
      "outputs": [],
      "source": [
        "# Defining a function to define data loaders\n",
        "# Found batch size 128 to be the optimal value based on testing\n",
        "def create_dataloaders(train_data, test_data,b_train=128,b_test=32):\n",
        "    train_loader = DataLoader(train_data, batch_size=b_train, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=b_test, shuffle=False)\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NaN6HhGbA8tj"
      },
      "outputs": [],
      "source": [
        "# define the train and test sets\n",
        "train_loader, test_loader = create_dataloaders(train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Cr5HHGHtSzS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dc5983-3c04-45c1-b0f5-d3b6d7c9004b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 200MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Initializing the model\n",
        "from torchvision.models import resnet50\n",
        "model = resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs,2)\n",
        "\n",
        "model = model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SESFpj7B9a21"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer and scehduler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PrJSOIR9mAh",
        "outputId": "a1bf8f00-d0e8-4ba3-fd0e-17800d560ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.3271, Accuracy: 87.48%\n",
            "Epoch 2/50, Loss: 0.1504, Accuracy: 93.68%\n",
            "Epoch 3/50, Loss: 0.1194, Accuracy: 95.57%\n",
            "Epoch 4/50, Loss: 0.0731, Accuracy: 97.69%\n",
            "Epoch 5/50, Loss: 0.0589, Accuracy: 98.12%\n",
            "Epoch 6/50, Loss: 0.0535, Accuracy: 98.18%\n",
            "Epoch 7/50, Loss: 0.0635, Accuracy: 97.63%\n",
            "Epoch 8/50, Loss: 0.0461, Accuracy: 98.42%\n",
            "Epoch 9/50, Loss: 0.0353, Accuracy: 98.72%\n",
            "Epoch 10/50, Loss: 0.0274, Accuracy: 99.15%\n",
            "Epoch 11/50, Loss: 0.0205, Accuracy: 99.45%\n",
            "Epoch 12/50, Loss: 0.0137, Accuracy: 99.64%\n",
            "Epoch 13/50, Loss: 0.0093, Accuracy: 99.88%\n",
            "Epoch 14/50, Loss: 0.0106, Accuracy: 99.57%\n",
            "Epoch 15/50, Loss: 0.0111, Accuracy: 99.51%\n",
            "Epoch 16/50, Loss: 0.0077, Accuracy: 99.70%\n",
            "Epoch 17/50, Loss: 0.0073, Accuracy: 99.70%\n",
            "Epoch 18/50, Loss: 0.0059, Accuracy: 99.82%\n",
            "Epoch 19/50, Loss: 0.0072, Accuracy: 99.76%\n",
            "Epoch 20/50, Loss: 0.0051, Accuracy: 99.64%\n",
            "Epoch 21/50, Loss: 0.0057, Accuracy: 99.70%\n",
            "Epoch 22/50, Loss: 0.0050, Accuracy: 99.82%\n",
            "Epoch 23/50, Loss: 0.0045, Accuracy: 99.94%\n",
            "Epoch 24/50, Loss: 0.0055, Accuracy: 99.76%\n",
            "Epoch 25/50, Loss: 0.0050, Accuracy: 99.70%\n",
            "Epoch 26/50, Loss: 0.0073, Accuracy: 99.76%\n",
            "Epoch 27/50, Loss: 0.0064, Accuracy: 99.76%\n",
            "Epoch 28/50, Loss: 0.0065, Accuracy: 99.64%\n",
            "Epoch 29/50, Loss: 0.0044, Accuracy: 99.88%\n",
            "Epoch 30/50, Loss: 0.0047, Accuracy: 99.76%\n",
            "Epoch 31/50, Loss: 0.0051, Accuracy: 99.76%\n",
            "Epoch 32/50, Loss: 0.0050, Accuracy: 99.82%\n",
            "Epoch 33/50, Loss: 0.0058, Accuracy: 99.70%\n",
            "Epoch 34/50, Loss: 0.0092, Accuracy: 99.57%\n",
            "Epoch 35/50, Loss: 0.0052, Accuracy: 99.82%\n",
            "Epoch 36/50, Loss: 0.0061, Accuracy: 99.76%\n",
            "Epoch 37/50, Loss: 0.0051, Accuracy: 99.82%\n",
            "Epoch 38/50, Loss: 0.0057, Accuracy: 99.70%\n",
            "Epoch 39/50, Loss: 0.0057, Accuracy: 99.70%\n",
            "Epoch 40/50, Loss: 0.0054, Accuracy: 99.70%\n",
            "Epoch 41/50, Loss: 0.0054, Accuracy: 99.76%\n",
            "Epoch 42/50, Loss: 0.0040, Accuracy: 99.82%\n",
            "Epoch 43/50, Loss: 0.0053, Accuracy: 99.88%\n",
            "Epoch 44/50, Loss: 0.0066, Accuracy: 99.70%\n",
            "Epoch 45/50, Loss: 0.0051, Accuracy: 99.82%\n",
            "Epoch 46/50, Loss: 0.0054, Accuracy: 99.88%\n",
            "Epoch 47/50, Loss: 0.0071, Accuracy: 99.70%\n",
            "Epoch 48/50, Loss: 0.0044, Accuracy: 99.82%\n",
            "Epoch 49/50, Loss: 0.0068, Accuracy: 99.51%\n",
            "Epoch 50/50, Loss: 0.0050, Accuracy: 99.76%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _,predicted = torch.max(outputs.data,1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhew6HeN9pHI"
      },
      "outputs": [],
      "source": [
        "# Used to evaluate test data if data is split into train and test\n",
        "def evaluate_test(test_loader):\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "    test_accuracy = 100 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IX7OuBETCk2g"
      },
      "outputs": [],
      "source": [
        "# Define a function to predict the outputs for test data\n",
        "results = {}\n",
        "def predict_op():\n",
        "  global res\n",
        "  global results\n",
        "  res[\"emergency_or_not\"].clear()\n",
        "  model.eval()\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "  with torch.no_grad():\n",
        "      for inputs in test_loader:\n",
        "          inputs = inputs.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          # print([t.item() for t in list(predicted)])\n",
        "          res[\"emergency_or_not\"].extend([t.item() for t in list(predicted)])\n",
        "  results = {}\n",
        "  for i in range(len(res[\"image_names\"])):\n",
        "    results[res[\"image_names\"][i]] = res[\"emergency_or_not\"][i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zhVcAV7eCqAB"
      },
      "outputs": [],
      "source": [
        "# Define a function to create csv\n",
        "def create_csv(name):\n",
        "  temp = []\n",
        "  import pandas as pd\n",
        "  data = pd.read_csv(\"/content/Qualcomm-DL-Hackathon/test.csv\")\n",
        "  for i in list(data[\"image_names\"]):\n",
        "    temp.append(results[i])\n",
        "  data[\"emergency_or_not\"] = temp\n",
        "  data.to_csv(f\"{name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BT2SWerwA8tk"
      },
      "outputs": [],
      "source": [
        "# Predict the outputs and create the final csv file\n",
        "predict_op()\n",
        "create_csv(\"submission_amugalih\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1hnIhy59dPo"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}